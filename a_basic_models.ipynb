{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQbywCfqQxOvA6G9TB7We0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yibei990826/author_identification/blob/main/a_basic_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeADd2gc7GsM",
        "outputId": "d767751e-10d6-4e93-a6f9-efba48a63b93"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nFCsuVAYem_V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
        "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
        "    :param actual: Array containing the actual target classes\n",
        "    :param predicted: Matrix with class predictions, one probability per class\n",
        "    \"\"\"\n",
        "    # Convert 'actual' to a binary array if it's not already:\n",
        "    if len(actual.shape) == 1:\n",
        "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
        "        for i, val in enumerate(actual):\n",
        "            actual2[i, val] = 1\n",
        "        actual = actual2\n",
        "\n",
        "    clip = np.clip(predicted, eps, 1 - eps)\n",
        "    rows = actual.shape[0]\n",
        "    vsota = np.sum(actual * np.log(clip))\n",
        "    return -1.0 / rows * vsota"
      ],
      "metadata": {
        "id": "Z811xUx5EgtL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_id = '14G3JoM_1Sa0wth9lZGpxtgThs5XIWGBk'\n",
        "train_id = '1wImpqT63U1Hlip9PKtO0R_f-iAqzHqpq'\n",
        "\n",
        "url_test = 'https://drive.google.com/uc?id={}'.format(test_id)\n",
        "url_train = 'https://drive.google.com/uc?id={}'.format(train_id)\n",
        "\n",
        "train = pd.read_csv(url_train)\n",
        "test = pd.read_csv(url_test)"
      ],
      "metadata": {
        "id": "ZiRA2YFl2QNe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bop6tx1x6zWI",
        "outputId": "df2a811d-00fe-4ec2-fd9a-1ea6d9f3baeb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19579, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['author'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQY5jKKV-up-",
        "outputId": "b3a59006-5d7f-4f6d-9a65-4645c73929ee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EAP    7900\n",
              "MWS    6044\n",
              "HPL    5635\n",
              "Name: author, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use labelEncoder to transform labels in to numerics\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y = lbl_enc.fit_transform(train.author.values)"
      ],
      "metadata": {
        "id": "rUvQzWGB7Qt1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train['text'].values"
      ],
      "metadata": {
        "id": "SEm4C_-U97kj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
        "                                                  stratify = y,\n",
        "                                                  random_state=42,\n",
        "                                                  test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "663kP7NM-evX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic Model:\n"
      ],
      "metadata": {
        "id": "iZJk74xP_KRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Tf-idf + Logistic Regression"
      ],
      "metadata": {
        "id": "z1EFvl5aJwaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate the TFVectorizer and vectorize the text\n",
        "vectorizer = TfidfVectorizer(max_features=None,\n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
        "            stop_words = 'english')\n",
        "\n",
        "X_tf = vectorizer.fit_transform(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s95uvu62_A6C",
        "outputId": "c11b2422-8e7f-4fde-c727-60f54c13a30f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py:558: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the vectorized text into train and validation set\n",
        "X_train_tf, X_val_tf, y_train, y_val = train_test_split(X_tf, y,\n",
        "                                                  stratify = y,\n",
        "                                                  random_state=42,\n",
        "                                                  test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "OPvO7Ec2C5wj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use logistic Regression model to distinguish\n",
        "clf = LogisticRegression(C=1.0)\n",
        "clf.fit(X_train_tf, y_train)\n",
        "\n",
        "predictions = clf.predict_proba(X_val_tf)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_val, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfzUwuQFDZyx",
        "outputId": "12589ba2-78fa-4d6c-bd79-fb5f23cf485c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logloss: 0.768 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Counter vectorizer"
      ],
      "metadata": {
        "id": "1BAfK5XEH6YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate the CountVectorizer and vectorize the text\n",
        "vectorizer = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), stop_words = 'english')\n",
        "\n",
        "X_tf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the vectorized text into train and validation set\n",
        "X_train_tf, X_val_tf, y_train, y_val = train_test_split(X_tf, y,\n",
        "                                                  stratify = y,\n",
        "                                                  random_state=42,\n",
        "                                                  test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "4F8jRkD4IOcZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(C=1.0)\n",
        "clf.fit(X_train_tf, y_train)\n",
        "\n",
        "predictions = clf.predict_proba(X_val_tf)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_val, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-fcRQ-GEKjX",
        "outputId": "29f4a526-79d4-409a-f1ba-788b432a95fb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logloss: 0.527 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Naive Bayes classifier"
      ],
      "metadata": {
        "id": "_g4nLs8uJOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting a simple Naive Bayes on Counts\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_tf, y_train)\n",
        "predictions = clf.predict_proba(X_val_tf)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_val, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKWO9kG2JSZI",
        "outputId": "78cfe57e-69bc-4d78-ee56-cb9f28552818"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logloss: 0.485 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4 SVM Classifier"
      ],
      "metadata": {
        "id": "scCzdRlTJsnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
        "svd = decomposition.TruncatedSVD(n_components=120)\n",
        "svd.fit(X_train_tf)\n",
        "xtrain_svd = svd.transform(X_train_tf)\n",
        "xvalid_svd = svd.transform(X_val_tf)\n",
        "\n",
        "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
        "scl = preprocessing.StandardScaler()\n",
        "scl.fit(xtrain_svd)\n",
        "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
        "xvalid_svd_scl = scl.transform(xvalid_svd)"
      ],
      "metadata": {
        "id": "tlRikTo5J52L"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting a simple SVM\n",
        "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
        "clf.fit(xtrain_svd_scl, y_train)\n",
        "predictions = clf.predict_proba(xvalid_svd_scl)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_val, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zimL-pCTKhjV",
        "outputId": "a59cb26c-949d-4bdb-f088-c99553ec0ea1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logloss: 0.783 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5 XGB Classifier"
      ],
      "metadata": {
        "id": "-v2CttFWNOcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting a simple xgboost on tf-idf svd features\n",
        "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "clf.fit(xtrain_svd, y_train)\n",
        "predictions = clf.predict_proba(xvalid_svd)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_val, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyKZvJwlNSVX",
        "outputId": "b7679338-4564-452e-c51b-847a2f43066e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logloss: 0.792 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.6 Grid Search"
      ],
      "metadata": {
        "id": "66FuGK6HOp4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# user make_scorer to d\n",
        "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
      ],
      "metadata": {
        "id": "P8jkPj-GOsdX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline\n",
        "svd = TruncatedSVD()\n",
        "scl = preprocessing.StandardScaler()\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "clf = pipeline.Pipeline([('svd', svd),\n",
        "                         ('scl', scl),\n",
        "                         ('lr', lr_model)])\n",
        "param_grid = {'svd__n_components' : [120, 180],\n",
        "              'lr__C': [0.1, 1.0, 10],\n",
        "              'lr__penalty': ['l1', 'l2']}"
      ],
      "metadata": {
        "id": "YWyoLtBRPZGP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Grid Search Model\n",
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
        "                                 verbose=10, n_jobs=-1, cv=2)\n",
        "\n",
        "# Fit Grid Search Model\n",
        "model.fit(X_train_tf, y_train)\n",
        "\n",
        "print(\"Best score: %0.3f\" % model.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = model.best_estimator_.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2u1Y9yJP5Ra",
        "outputId": "c5e462bd-d71c-4c95-a3b6-b1c4e481e0fd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "12 fits failed out of a total of 24.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "12 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan         nan -0.80004595 -0.761596           nan         nan\n",
            " -0.80378413 -0.75730567         nan         nan -0.80349962 -0.76032033]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score: -0.757\n",
            "Best parameters set:\n",
            "\tlr__C: 1.0\n",
            "\tlr__penalty: 'l2'\n",
            "\tsvd__n_components: 180\n"
          ]
        }
      ]
    }
  ]
}